{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import (\n",
    "    RunnableBranch,\n",
    "    RunnableLambda,\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import Tuple, List, Optional\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import os\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain.document_loaders import WikipediaLoader\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from neo4j import GraphDatabase\n",
    "from yfiles_jupyter_graphs import GraphWidget\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores.neo4j_vector import remove_lucene_chars\n",
    "from langchain_core.runnables import ConfigurableField, RunnableParallel, RunnablePassthrough\n",
    "\n",
    "try:\n",
    "  import google.colab\n",
    "  from google.colab import output\n",
    "  output.enable_custom_widget_manager()\n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"NEO4J_URI\"] = \"\"\n",
    "os.environ[\"NEO4J_USERNAME\"] = \"\"\n",
    "os.environ[\"NEO4J_PASSWORD\"] = \"\"\n",
    "\n",
    "graph = Neo4jGraph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "import json\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('/home/yu.zhiyin/CellRAG/data/total/total_train.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Number of chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import time\n",
    "from neo4j.exceptions import ServiceUnavailable\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "\n",
    "\n",
    "def process_batch(batch):\n",
    "    retry_attempts = 3\n",
    "    for attempt in range(retry_attempts):\n",
    "        try:\n",
    "            graph_documents = llm_transformer.convert_to_graph_documents(batch)\n",
    "            graph.add_graph_documents(\n",
    "                graph_documents,\n",
    "                baseEntityLabel=True,\n",
    "                include_source=True\n",
    "            )\n",
    "            print(\"Batch processed successfully.\")\n",
    "            return \n",
    "        except ServiceUnavailable as e:\n",
    "            print(f\"Service unavailable, attempt {attempt + 1} of {retry_attempts}: {e}\")\n",
    "            if attempt < retry_attempts - 1:\n",
    "                time.sleep(5)  \n",
    "            else:\n",
    "                print(\"Failed to process batch after several attempts.\")\n",
    "                raise  \n",
    "\n",
    "batch_size = 10  \n",
    "\n",
    "llm=ChatOpenAI(temperature=0, model_name=\"gpt-4-turbo\") \n",
    "llm_transformer = LLMGraphTransformer(llm=llm)\n",
    "\n",
    "start_batch = 0 \n",
    "for i in range(start_batch * batch_size, len(chunks), batch_size):\n",
    "    batch = chunks[i:i + batch_size]\n",
    "    process_batch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_cypher = \"MATCH (s)-[r:!MENTIONS]->(t) RETURN s,r,t LIMIT 50\"\n",
    "\n",
    "def showGraph(cypher: str = default_cypher):\n",
    "    driver = GraphDatabase.driver(\n",
    "        uri = os.environ[\"NEO4J_URI\"],\n",
    "        auth = (os.environ[\"NEO4J_USERNAME\"],\n",
    "                os.environ[\"NEO4J_PASSWORD\"]))\n",
    "    session = driver.session()\n",
    "    widget = GraphWidget(graph = session.run(cypher).graph())\n",
    "    widget.node_label_mapping = 'id'\n",
    "    return widget\n",
    "\n",
    "showGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = Neo4jVector.from_existing_graph(\n",
    "    OpenAIEmbeddings(),\n",
    "    search_type=\"hybrid\",\n",
    "    node_label=\"Document\",\n",
    "    text_node_properties=[\"text\"],\n",
    "    embedding_node_property=\"embedding\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "graph.query(\n",
    "    \"CREATE FULLTEXT INDEX entity IF NOT EXISTS FOR (e:__Entity__) ON EACH [e.id]\"\n",
    ")\n",
    "\n",
    "class Entities(BaseModel):\n",
    "    \"\"\"Identifying information about entities.\"\"\"\n",
    "    names: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"All Tissue and top 100 gene entities that appear in the text.\",\n",
    "    )\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are extracting Tissue and top 100 gene entities from the text.\"),\n",
    "        (\"human\", \"Extract the following information from the input: {question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "entity_chain = prompt | llm.with_structured_output(Entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_chain.invoke({\"question\": \"Task: Given the following information about a cell, predict its most likely cell type. Provide only the single most probable cell type without any additional explanation. Tissue: heart left ventricle. Top 100 genes for this cell (highest expression first): MALAT1, RYR2, TTN, LINC02388, DMD, SLC8A1, FHL2, THSD4, TTN-AS1, RBM20, CACNB2, LAMA2, SORBS2, PRKG1, CTNNA3, RP11-362K2.2, ABLIM1, MLIP, MARK3, PRKN, FHOD3, IL1RAPL1, DAPK2, RNF150, CD36, PDLIM5, PHACTR1, QKI, CUX1, CACNA1C, TNNT2, AC011288.2, AKAP13, PDZRN3, CDIN1, CFLAR, LDB3, NEAT1, PDE3A, SORBS1, TXNIP, MYL2, PTPRK, WDPCP, SLC1A3, NDRG3, LARGE1, ATXN1, FHIT, GALNT17, ATP1A2, GPHN, ELL2, CRADD, TMEM117, FANCC, DTNA, PDE4DIP, PLEKHA5, ZNF721, FOXO1, PALLD, JMJD1C, LRMDA, OBSCN, RP11-499P20.2, SLC8A1-AS1, EXOC6B, LRRTM3, TECRL, RASSF3, NEBL, DANT2, SVIL, PDK4, ANKRD17, REV3L, MYH7, PDE1C, FBXL7, ARL15, RBMS3, ACTC1, PLCL1, MSRB3, PKP2, CH17-189H20.1, SDK1, AKAP6, EXOC4, EXOC6, GAPVD1, ENSG00000273748, MAGI1, EPHA4, AKAP9, UBE2E2, USP49, MEF2A, RWDD1.\"}).names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_full_text_query(input: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a full-text search query for a given input string.\n",
    "\n",
    "    This function constructs a query string suitable for a full-text search.\n",
    "    It processes the input string by splitting it into words and appending a\n",
    "    similarity threshold (~2 changed characters) to each word, then combines\n",
    "    them using the AND operator. Useful for mapping entities from user questions\n",
    "    to database values, and allows for some misspelings.\n",
    "    \"\"\"\n",
    "    full_text_query = \"\"\n",
    "    words = [el for el in remove_lucene_chars(input).split() if el]\n",
    "    for word in words[:-1]:\n",
    "        full_text_query += f\" {word} AND\"\n",
    "    full_text_query += f\" {words[-1]}\"\n",
    "    return full_text_query.strip()\n",
    "\n",
    "def structured_retriever(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Collects the neighborhood of entities mentioned in the question.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    entities_org = entity_chain.invoke({\"question\": question})\n",
    "    ##entities = entities_org.names[:11]\n",
    "    ##print(f\"Extracted entities: {entities}\")  # Debugging\n",
    "    \n",
    "    for entity in entities_org.names:\n",
    "        query_str = generate_full_text_query(entity)\n",
    "        #print(f\"Generated query: {query_str}\")  # Debugging\n",
    "        \n",
    "        response = graph.query(\n",
    "            \"\"\"CALL db.index.fulltext.queryNodes('entity', $query, {limit:2})\n",
    "            YIELD node,score\n",
    "            CALL {\n",
    "              WITH node\n",
    "              MATCH (node)-[r:!MENTIONS]->(neighbor)\n",
    "              RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\n",
    "              UNION ALL\n",
    "              WITH node\n",
    "              MATCH (node)<-[r:!MENTIONS]-(neighbor)\n",
    "              RETURN neighbor.id + ' - ' + type(r) + ' -> ' + node.id AS output\n",
    "            }\n",
    "            RETURN output LIMIT 50\n",
    "            \"\"\",\n",
    "            {\"query\": query_str},\n",
    "        )\n",
    "        \n",
    "      #  if not response:\n",
    "         #   print(f\"No results for query: {query_str}\")  # Debugging\n",
    "            \n",
    "        result.extend([el['output'] for el in response])\n",
    "    \n",
    "    return \"\\n\".join(result)\n",
    "print(structured_retriever(\"Given the following information about a cell, predict two candidate cell types. Provide only the cell types without additional explanation. Tissue: pancreas. Top 100 genes for this cell (highest expression first): SST, SERPINA1, GNAS, PCSK1N, RBP4, CHGA, RPL3, ACTG1, EEF1A1, TPT1, RPL19, CHGB, HLA-A, HSPA1A, CPE, RPL41, SCG5, EDN3, RPS4X, RPL8, RPL37A, TUBA1B, DYNLL1, RPL7A, GAD2, RPS8, RPL27A, RPS11, B2M, TIMP1, PTPRN, RPS2, RPL15, CD63, RPS15, TTR, RPL13A, SCG2, AQP3, IDS, PCSK2, RPS3A, RPL23A, GPX3, RPL10, TUBA1A, FOS, H3F3A, SEC11C, SERF2, RPS27A, EMC10, SCGN, RPS12, GAPDH, H3F3B, TAGLN2, NLRP1, RPL13, RPL14, PEG10, RPS14, RPS9, RPL24, ZFP36, RPS24, JUNB, RPS23, RPS28, EIF1, FAU, RPL11, FTH1, CLU, ATP5E, CALY, TMSB4X, RPL18, RPS29, RPL35A, FTL, PSAP, ENO1, RPL23, RPS18, DHRS2, RPLP2, RPS19, S100A6, MIF, RPLP1, HSP90AA1, RNASEK, CHCHD2, SSR4, RPL6, RPL28, HSPA5, HINT1, MALAT1.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unstructured_retriever(question: str):\n",
    "    search_results = vector_index.similarity_search(question,k=2)\n",
    "    \n",
    "    unstructured_data = [el.page_content for el in search_results]\n",
    "    \n",
    "    return unstructured_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def merge_structured_data(structured_data: str) -> str:\n",
    "    lines = structured_data.strip().split('\\n')\n",
    "    ##print(lines)\n",
    "    merged_located = defaultdict(list)\n",
    "    merged_expressed = defaultdict(list)\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if \"LOCATED_IN\" in line:\n",
    "            key, value = line.split(' - LOCATED_IN')\n",
    "            key = key.strip()\n",
    "            value = value.strip()\n",
    "            merged_located[value].append(key)  \n",
    "        elif \"EXPRESSED_IN\" in line:\n",
    "            key, value = line.split('->')\n",
    "            key = key.strip()\n",
    "            value = value.strip()\n",
    "            merged_expressed[key].append(value) \n",
    "\n",
    "    merged_lines = []\n",
    "    \n",
    "    for value, keys in merged_located.items():\n",
    "        unique_keys = list(set(keys)) \n",
    "        merged_line = f\"{', '.join(unique_keys)} - LOCATED_IN {value.strip()}\"\n",
    "        merged_lines.append(merged_line)\n",
    "\n",
    "    for key, values in merged_expressed.items():\n",
    "        unique_values = list(set(values))  \n",
    "        merged_line = f\"{key} -> {', '.join(unique_values)}\"\n",
    "        merged_lines.append(merged_line)\n",
    "\n",
    "    return '\\n'.join(merged_lines)\n",
    "\n",
    "def retriever(question: str):\n",
    "    print(f\"Search query: {question}\")\n",
    "    structured_data = structured_retriever(question)\n",
    "    merged_structured_data = merge_structured_data(structured_data)\n",
    "    \n",
    "    unstructured_data = [el.page_content for el in vector_index.similarity_search(question,k=4)]\n",
    "    final_data = f\"\"\"Structured data:\n",
    "{merged_structured_data}  \n",
    "Unstructured data:\n",
    "{\"#Document \".join(unstructured_data)}\n",
    "    \"\"\"\n",
    "    return final_data\n",
    "\n",
    "\n",
    "# question = \"Given the following information about a cell, predict two candidate cell types. Provide only the cell types without additional explanation. Tissue: pancreas. Top 100 genes for this cell (highest expression first): SST, SERPINA1, GNAS, PCSK1N, RBP4, CHGA, RPL3, ACTG1, EEF1A1, TPT1, RPL19, CHGB, HLA-A, HSPA1A, CPE, RPL41, SCG5, EDN3, RPS4X, RPL8, RPL37A, TUBA1B, DYNLL1, RPL7A, GAD2, RPS8, RPL27A, RPS11, B2M, TIMP1, PTPRN, RPS2, RPL15, CD63, RPS15, TTR, RPL13A, SCG2, AQP3, IDS, PCSK2, RPS3A, RPL23A, GPX3, RPL10, TUBA1A, FOS, H3F3A, SEC11C, SERF2, RPS27A, EMC10, SCGN, RPS12, GAPDH, H3F3B, TAGLN2, NLRP1, RPL13, RPL14, PEG10, RPS14, RPS9, RPL24, ZFP36, RPS24, JUNB, RPS23, RPS28, EIF1, FAU, RPL11, FTH1, CLU, ATP5E, CALY, TMSB4X, RPL18, RPS29, RPL35A, FTL, PSAP, ENO1, RPL23, RPS18, DHRS2, RPLP2, RPS19, S100A6, MIF, RPLP1, HSP90AA1, RNASEK, CHCHD2, SSR4, RPL6, RPL28, HSPA5, HINT1, MALAT1.\"\n",
    "\n",
    "# result = retriever(question)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Condense a chat history and follow-up question into a standalone question\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question,\n",
    "in its original language.\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"  # noqa: E501\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "\n",
    "def _format_chat_history(chat_history: List[Tuple[str, str]]) -> List:\n",
    "    buffer = []\n",
    "    for human, ai in chat_history:\n",
    "        buffer.append(HumanMessage(content=human))\n",
    "        buffer.append(AIMessage(content=ai))\n",
    "    return buffer\n",
    "\n",
    "_search_query = RunnableBranch(\n",
    "    # If input includes chat_history, we condense it with the follow-up question\n",
    "    (\n",
    "        RunnableLambda(lambda x: bool(x.get(\"chat_history\"))).with_config(\n",
    "            run_name=\"HasChatHistoryCheck\"\n",
    "        ),  # Condense follow-up question and chat into a standalone_question\n",
    "        RunnablePassthrough.assign(\n",
    "            chat_history=lambda x: _format_chat_history(x[\"chat_history\"])\n",
    "        )\n",
    "        | CONDENSE_QUESTION_PROMPT\n",
    "        | ChatOpenAI(temperature=0)\n",
    "        | StrOutputParser(),\n",
    "    ),\n",
    "    # Else, we have no chat history, so just pass through the question\n",
    "    RunnableLambda(lambda x : x[\"question\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Use natural language and be concise.\n",
    "Answer:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "    RunnableParallel(\n",
    "        {\n",
    "            \"context\": _search_query | retriever,\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "    )\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
    "from requests.exceptions import ConnectionError\n",
    "\n",
    "n_start = 800  \n",
    "n_end = 900  \n",
    "\n",
    "with open('/home/yu.zhiyin/CellRAG/data/total/total_test_task1.txt', 'r') as question_file, open('cell_type_predictions12_add2.txt', 'a+') as output_file:\n",
    "    questions = question_file.readlines()\n",
    "\n",
    "    for question in questions[n_start:n_end]:  \n",
    "        try:\n",
    "            response = chain.invoke({\n",
    "                \"question\": question.strip()  \n",
    "            })\n",
    "            \n",
    "            output_file.write(f\"{response}\\n\")  \n",
    "        except ConnectionError as e:\n",
    "            print(f\"API connection error {e}\")\n",
    "            output_file.write(\"API connection error\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"error: {e}\")\n",
    "            output_file.write(\"error\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "def read_top100_genes(file_path):\n",
    "    top100_genes_list = []\n",
    "    with open(file_path, mode='r') as file:\n",
    "        for line in file:\n",
    "            if \"Top 100 genes for this cell\" in line:\n",
    "                genes_part = line.split(\"Top 100 genes for this cell (highest expression first): \")[1].strip()\n",
    "                top100_genes_list.append(genes_part)\n",
    "    return top100_genes_list\n",
    "\n",
    "def read_cell_types(file_path):\n",
    "    cell_types_list = []\n",
    "    with open(file_path, mode='r') as file:\n",
    "        for line in file:\n",
    "            types = line.strip().lower()\n",
    "            cell_types_list.append(types)\n",
    "    return cell_types_list\n",
    "\n",
    "def get_marker_sentence(cell_types, cell_marker_dict):\n",
    "    marker_sentences = []\n",
    "    for cell_type in cell_types.split(', '): \n",
    "        normalized_cell_type = cell_type.strip().lower()\n",
    "     ##   print(f\"Normalized cell type: {normalized_cell_type}\")\n",
    "        if normalized_cell_type in cell_marker_dict:\n",
    "            markers = ', '.join(cell_marker_dict[normalized_cell_type])\n",
    "        else:\n",
    "            markers = \"unknown\"  \n",
    "       ## print(f\"Cell type: {cell_type}, Markers: {markers}\")\n",
    "        marker_sentences.append(f\"{cell_type}: {markers}\")\n",
    "    return \". \".join(marker_sentences)\n",
    "\n",
    "def remove_duplicates(cell_types):\n",
    "    cell_types_set = set(cell_types.split(', '))\n",
    "    return ', '.join(cell_types_set)\n",
    "\n",
    "def read_total_marker(file_path):\n",
    "    cell_marker_dict = {}\n",
    "    with open(file_path, mode='r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        headers = next(reader) \n",
    "        for row in reader:\n",
    "            if row:  \n",
    "                for i, marker in enumerate(row):\n",
    "                    key = headers[i].strip().lower()\n",
    "                    marker = marker.strip()\n",
    "                    if key not in cell_marker_dict:\n",
    "                        cell_marker_dict[key] = []\n",
    "                    if marker: \n",
    "                        cell_marker_dict[key].append(marker)\n",
    "    return cell_marker_dict\n",
    "\n",
    "llm=ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\") \n",
    "\n",
    "top100_genes_list = read_top100_genes('/home/yu.zhiyin/CellRAG/data/total/total_test_task2.txt')\n",
    "predicted_cell_types_list = read_cell_types('/home/yu.zhiyin/CellRAG/data/cell_type_predictions12_add.txt')\n",
    "retrieved_cell_types_list = read_cell_types('/home/yu.zhiyin/CellRAG/data/similar_cell_types111.txt')\n",
    "\n",
    "cell_marker_dict = read_total_marker('/home/yu.zhiyin/CellRAG/data/marker gene/total.csv')\n",
    "\n",
    "template = \"\"\"\n",
    "Given the following information about a cell:\n",
    "Top 100 genes: {top100_genes}.\n",
    "Candidate cell types and their marker genes: {predicted_markers}.\n",
    "Similar cell types retrieved and their marker genes: {retrieved_markers}.\n",
    "\n",
    "Task: Given the following information about a cell, predict its most likely cell type. Provide only the single most probable cell type without any additional explanation.\n",
    "From the following cell types, select the most probable: 'fibroblast', 'activated CD4-positive, alpha-beta T cell', 'HSPCs', 'ductal', 'regular ventricular cardiac myocyte', 'vein endothelial cell', 'Erythrocytes', 'endothelial cell of artery', 'acinar', 'beta', 'B cell', 'natural killer cell', 'CD4-positive, alpha-beta cytotoxic T cell', 'epicardial adipocyte', 'regular atrial cardiac myocyte', 'macrophage', 'Plasmacytoid dendritic cells', 'gamma', 'native cell', 'smooth muscle cell', 'endothelial', 'CD20+ B cells', 'neural cell', 'Megakaryocyte progenitors', 'Plasma cells', 'endothelial cell', 'CD4+ T cells', 'CD10+ B cells', 'Monocyte-derived dendritic cells', 'CD14+ Monocytes', 'delta', 'Erythroid progenitors', 'activated CD8-positive, alpha-beta T cell', 'NK cells', 'mature NK T cell', 'alpha', 'CD8-positive, alpha-beta cytotoxic T cell', 'monocyte', 'pericyte cell', 'capillary endothelial cell', 'NKT cells', 'CD14-positive, CD16-positive monocyte', 'CD8+ T cells', 'Monocyte progenitors'.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"top100_genes\", \"predicted_markers\", \"retrieved_markers\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "# 定义起始和结束索引\n",
    "start_index = 0  \n",
    "end_index = 900   \n",
    "\n",
    "with open('/home/yu.zhiyin/CellRAG/data/llm_predictions_whole_graph_gpt4omini.txt', mode='w') as file:\n",
    "\n",
    "    for i in range(start_index, min(end_index, len(top100_genes_list))):\n",
    "        top100_genes = top100_genes_list[i]\n",
    "        predicted_cell_types = predicted_cell_types_list[i]\n",
    "        retrieved_cell_types = retrieved_cell_types_list[i]\n",
    "        \n",
    "        unique_predicted_cell_types = remove_duplicates(predicted_cell_types)\n",
    "        unique_retrieved_cell_types = remove_duplicates(retrieved_cell_types)\n",
    "        \n",
    "        predicted_markers_sentence = get_marker_sentence(unique_predicted_cell_types, cell_marker_dict)\n",
    "        retrieved_markers_sentence = get_marker_sentence(unique_retrieved_cell_types, cell_marker_dict)\n",
    "\n",
    "        filled_prompt = prompt.format(\n",
    "            top100_genes=top100_genes,\n",
    "            predicted_markers=predicted_markers_sentence,\n",
    "            retrieved_markers=retrieved_markers_sentence\n",
    "        )\n",
    "\n",
    "        chain = LLMChain(llm=llm, prompt=prompt)\n",
    "        response = chain.run({\n",
    "            \"top100_genes\": top100_genes,\n",
    "            \"predicted_markers\": predicted_markers_sentence,\n",
    "            \"retrieved_markers\": retrieved_markers_sentence\n",
    "        }).strip()  \n",
    "\n",
    "        file.write(f\"{response}\\n\")\n",
    "        \n",
    "        print(f\"Cell {i+1}: Response saved to file.\")\n",
    "\n",
    "    print(\"All responses have been saved to llm_predictions.txt.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "singlecell",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
